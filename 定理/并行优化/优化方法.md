### 线程束内没有分支

https://face2ai.com/CUDA-F-3-2-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8-P1/

```
__global__ void mathKernel1(float *c)
{
	int tid = blockIdx.x* blockDim.x + threadIdx.x;

	float a = 0.0;
	float b = 0.0;
	if (tid % 2 == 0)a = 100.0f;
	else b = 200.0f;
	c[tid] = a + b;
}
```

这个函数，奇数的线程和偶数线程执行的if else不同，资源浪费。

改进

```
if ((tid/warpSize) % 2 == 0)
```

第一个线程束内的线程编号tid从0到31，tid/warpSize都等于0，那么就都执行if语句。
第二个线程束内的线程编号tid从32到63，tid/warpSize都等于1，执行else
线程束内没有分支，效率较高。

A trivial example is when the controlling condition depends only on (threadIdx / WSIZE) where WSIZE is the warp size.  

### 循环展开

https://face2ai.com/CUDA-F-3-5-%E5%B1%95%E5%BC%80%E5%BE%AA%E7%8E%AF/

```
for (int i=0;i<100;i++)
{
    a[i]=b[i]+c[i];
}
这个是最传统的写法，这个写法在各种c++教材上都能看到，不做解释，如果我们进行循环展开呢？

for (int i=0;i<100;i+=4)
{
    a[i+0]=b[i+0]+c[i+0];
    a[i+1]=b[i+1]+c[i+1];
    a[i+2]=b[i+2]+c[i+2];
    a[i+3]=b[i+3]+c[i+3];
}
```

此时reduction中也可以用

https://github.com/Tony-Tan/CUDA_Freshman/blob/master/12_reduce_unrolling/reduceUnrolling.cu

### 循环展开的倒金字塔式优化

https://face2ai.com/CUDA-F-3-2-%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8-P1/

### 优化数据传输效率

https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/

CPU默认数据收集是分页的，但是GPU不能直接访问分页内存。所以当GPU要获取CPU上的传输数据时，CUDA首先锁页，or "pinned"，array。所以首先是从分页内存传到pinned memory上，pinned memory也在Host上。然后再从pinned memory传到device上。

所以，我们可以直接用cudaMallocHost直接收集pinned host上的内存，效率几乎是之前的两倍

```
#include <stdio.h>
#include <assert.h>

// Convenience function for checking CUDA runtime API results
// can be wrapped around any runtime API call. No-op in release builds.
inline
cudaError_t checkCuda(cudaError_t result)
{
#if defined(DEBUG) || defined(_DEBUG)
    if (result != cudaSuccess) {
        fprintf(stderr, "CUDA Runtime Error: %s\n",
            cudaGetErrorString(result));
        assert(result == cudaSuccess);
    }
#endif
    return result;
}

void profileCopies(float* h_a,
    float* h_b,
    float* d,
    unsigned int  n,
    char* desc)
{
    printf("\n%s transfers\n", desc);

    unsigned int bytes = n * sizeof(float);

    // events for timing
    cudaEvent_t startEvent, stopEvent;
	float etime;
    cudaEventCreate(&startEvent);
    cudaEventCreate(&stopEvent);
    
    cudaEventRecord(startEvent, 0);
    checkCuda(cudaMemcpy(d, h_a, bytes, cudaMemcpyHostToDevice));
    cudaEventRecord(stopEvent, 0);
    cudaEventSynchronize(stopEvent);
    cudaEventElapsedTime(&etime, startEvent, stopEvent);
    printf(" time %f\n",  etime);

    checkCuda(cudaEventRecord(startEvent, 0));
    checkCuda(cudaMemcpy(h_b, d, bytes, cudaMemcpyDeviceToHost));
    checkCuda(cudaEventRecord(stopEvent, 0));
    checkCuda(cudaEventSynchronize(stopEvent));

    checkCuda(cudaEventElapsedTime(&time, startEvent, stopEvent));
    printf("  Device to Host bandwidth (GB/s): %f\n", bytes * 1e-6 / time);

    for (int i = 0; i < n; ++i) {
        if (h_a[i] != h_b[i]) {
            printf("*** %s transfers failed ***\n", desc);
            break;
        }
    }

    // clean up events
    checkCuda(cudaEventDestroy(startEvent));
    checkCuda(cudaEventDestroy(stopEvent));
}

int main()
{
    unsigned int nElements = 4 * 1024 * 1024;
    const unsigned int bytes = nElements * sizeof(float);

    // host arrays
    float* h_aPageable, * h_bPageable;
    float* h_aPinned, * h_bPinned;

    // device array
    float* d_a;

    // allocate and initialize
    h_aPageable = (float*)malloc(bytes);                    // host pageable
    h_bPageable = (float*)malloc(bytes);                    // host pageable
    checkCuda(cudaMallocHost((void**)&h_aPinned, bytes)); // host pinned
    checkCuda(cudaMallocHost((void**)&h_bPinned, bytes)); // host pinned
    checkCuda(cudaMalloc((void**)&d_a, bytes));           // device

    for (int i = 0; i < nElements; ++i) h_aPageable[i] = i;
    memcpy(h_aPinned, h_aPageable, bytes);
    memset(h_bPageable, 0, bytes);
    memset(h_bPinned, 0, bytes);

    // output device info and transfer size
    cudaDeviceProp prop;
    checkCuda(cudaGetDeviceProperties(&prop, 0));

    printf("\nDevice: %s\n", prop.name);
    printf("Transfer size (MB): %d\n", bytes / (1024 * 1024));

    // perform copies and report bandwidth
    profileCopies(h_aPageable, h_bPageable, d_a, nElements, "Pageable");
    profileCopies(h_aPinned, h_bPinned, d_a, nElements, "Pinned");

    printf("n");

    // cleanup
    cudaFree(d_a);
    cudaFreeHost(h_aPinned);
    cudaFreeHost(h_bPinned);
    free(h_aPageable);
    free(h_bPageable);

    return 0;
}
```

### 列主序

https://face2ai.com/CUDA-F-5-2-%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E7%9A%84%E6%95%B0%E6%8D%AE%E5%B8%83%E5%B1%80/

```
__global__ void setRowReadRow(int* out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;

    tile[threadIdx.y][threadIdx.x] = idx;
    __syncthreads();
    out[idx] = tile[threadIdx.y][threadIdx.x];
}
__global__ void setColReadCol(int* out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;

    tile[threadIdx.x][threadIdx.y] = idx;
    __syncthreads();
    out[idx] = tile[threadIdx.x][threadIdx.y];
}
```

存疑

### 共享内存

https://face2ai.com/CUDA-F-5-3-%E5%87%8F%E5%B0%91%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE/

使用共享内存来减少对全局内存的访问，全局内存即为传递到核函数的参数

然后在使用共享内存之前进行归约

https://github.com/CodedK/CUDA-by-Example-source-code-for-the-book-s-examples-/tree/master/chapter09

========基于CUDA的GPU并行程序设计，p302

关于imfilp的部分非常不错



```
PixBuffer[tid] = ImgScr[Idx];
PixBuffer[tid + 1] = ImgScr[Idx + 1];
PixBuffer[tid + 2] = ImgScr[Idx + 2];
__syncthreads();
```

PixBuffer是共享内存，但是几乎变化并不大，因为当写入共享内存的时候，它们以大端方式存储，其中较低的地址对应于高值字节，因此读取之后，共享内存存储了下面三个int

```
//PixBuffer:[B0 G0 R0 B1][G1 R1 B2 G2][R2 B3 G3 R3]
//global : [B3 G3 R3 B2][G2 R2 B1 G1][R1 B0 G0 R0]
```

这样我们的位置是混乱的，所以需要6个字节的交换操作

```
	// swap these 4 pixels inside Shared Memory
	SwapPtr = (uch *)(&PixBuffer[MYtid3]);      // [B0 G0 R0 B1] [G1 R1 B2 G2] [R2 B3 G3 R3]
	SWAP(SwapPtr[0], SwapPtr[9], SwapB)			// [B3 G0 R0 B1] [G1 R1 B2 G2] [R2 B0 G3 R3]
	SWAP(SwapPtr[1], SwapPtr[10], SwapB)		// [B3 G3 R0 B1] [G1 R1 B2 G2] [R2 B0 G0 R3]
	SWAP(SwapPtr[2], SwapPtr[11], SwapB)		// [B3 G3 R3 B1] [G1 R1 B2 G2] [R2 B0 G0 R0]
	SWAP(SwapPtr[3], SwapPtr[6], SwapB)			// [B3 G3 R3 B2] [G1 R1 B1 G2] [R2 B0 G0 R0]
	SWAP(SwapPtr[4], SwapPtr[7], SwapB)			// [B3 G3 R3 B2] [G2 R1 B1 G1] [R2 B0 G0 R0]
	SWAP(SwapPtr[5], SwapPtr[8], SwapB)			// [B3 G3 R3 B2] [G2 R2 B1 G1] [R1 B0 G0 R0]
```

但是效果仍然不是很好，所以

```
	// read 4 pixel blocks (12B = 3 int's) into 3 long registers
	A = ImgSrc32[MYsrcIndex];
	B = ImgSrc32[MYsrcIndex + 1];
	C = ImgSrc32[MYsrcIndex + 2];
	
	// Do the shuffling using these registers
	//NOW:		  A=[B1,R0,G0,B0]   B=[G2,B2,R1,G1]    C=[R3,G3,B3,R2]
	//OUR TARGET: D=[B2,R3,G3,B3]   E=[G1,B1,R2,G2]    F=[R0,G0,B1,R1]
	D = (C >> 8) | ((B << 8) & 0xFF000000);     // D=[B2,R3,G3,B3]
	E = (B << 24) | (B >> 24) | ((A >> 8) & 0x00FF0000) | ((C << 8) & 0x0000FF00);     // E=[G1,B1,R2,G2]
	F = ((A << 8) & 0xFFFF0000) | ((A >> 16) & 0x0000FF00) | ((B >> 8) & 0x000000FF);		// F=[R0,G0,B1,R1]

	//write the 4 pixels (3 int's) from Shared Memory into Global Memory
	ImgDst32[MYdstIndex] = D;
	ImgDst32[MYdstIndex + 1] = E;
	ImgDst32[MYdstIndex + 2] = F;
```

https://yangwc.com/2019/06/20/NbodySimulation/

局内存的访问速度非常慢，为了性能我们必须考虑这一点。共享内存仅限于同一个线程块内，不同线程块之间的线程没有共享内存这一概念。既然共享内存仅限于同一个线程块，我们就采用一个分块的策略，不同天体计算各自的加速度之间是有数据重用的，先将天体数据预先加载到共享内存，同一个线程块的线程访问这个共享内存做相应的数值计算。但是又引入一个问题，共享内存是有限的，所以对于一个线程块的共享内存，我们不是一次性地将所有天体数据从全局内存加载到共享内存，而是分批次地加载、计算。

=============CUDA高性能并行计算 p66

共享内存和SM相邻，并且提供了多达48KB可被同一线程块中所有线程高效访问的存储空间。

全局内存很慢，但是寄存器最快但也只能让单个线程使用。

### 横跨线程束

https://face2ai.com/CUDA-F-5-6-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%B4%97%E7%89%8C%E6%8C%87%E4%BB%A4/

### 常量内存

cuda by example An introduction

declaring memory as __const__ 可以加快访问速度，主要是因为

• A single read from constant memory can be broadcast to other “nearby” threads, effectively saving up to 15 reads.
• Constant memory is cached, so consecutive reads of the same address will not incur any additional memory traffic.  

At every line in your program, each thread in a warp executes the same instruction on different data.

这个时候就要提到half-warp的概念，当使用常量内存的时候，half-warp，也就是16个线程都读取这个线程，那么GPU只会产生a single read request，也就是内存负荷只有原来的1/16

存疑，更慢了

```


```

But the savings don’t stop at a 94 percent reduction in bandwidth when reading constant memory! Because we have committed to leaving the memory unchanged, the hardware can aggressively cache the constant data on the GPU. So after the first read from an address in constant memory, other half-warps
requesting the same address, and therefore hitting the constant cache, will generate no additional memory traffic.  

The trade-off to allowing the broadcast of a single read to 16 threads is that the
16 threads are allowed to place only a single read request at a time. For example,
if all 16 threads in a half-warp need different data from constant memory, the
16 different reads get serialized, effectively taking 16 times the amount of time
to place the request. If they were reading from conventional global memory, the
request could be issued at the same time. In this case, reading from constant
memory would probably be slower than using global memory  

意思是只有half warp只有并行读取常量内存的不同数据，才能省时间，如果这些线程同一时刻读取相同的数据的话，那么就比全局内存更慢。

### Stream

=====cuda by example An introduction

CUDA streams can play an important role in accelerating your applications.
A cudA stream represents a queue of GPU operations that get executed in a
specific order. We can add operations such as kernel launches, memory copies,
and event starts and stops into a stream.   

First, we will opt not to copy the input buffers in their entirety to the GPU. Rather, we will split our inputs into smaller chunks and perform the three-step process on each chunk. That is, we will take some fraction of the input buffers, copy them to the GPU, execute our kernel on that fraction of the buffers, and copy the resulting fraction of the output buffer back to the host. Imagine that we need   to do this because our GPU has much less memory than our host does, so the computation needs to be staged in chunks because the entire buffer can’t fit on the GPU at once. The code to perform this “chunkified” sequence of computations will look like this:  

之前是同步的，也就是synchronously，就是当函数返回的时候，函数里面的部分就都执行完成了。但现在可以用cudaMemcpyAsync，也就是异步的。The call to cudaMemcpyAsync()
simply places a request to perform a memory copy into the stream specified by
the argument stream.  当这个函数返回的时候，函数里面的内容可能都还没开始执行。

It is required that any host memory pointers passed to cudaMemcpyAsync() have been allocated by cudaHostAlloc(). That is, you are only allowed to schedule asynchronous copies to or from pagelocked memory.  

主要，就是有些核如果相互之间不影响，就可以异步传输数据，比如光线追踪，有限元也不影响，有限差分也不影响，这个三角形上算的什么值，和另一个三角形并无关系。除非像高斯滤波这样，需要全部数据都传输过去才能开始计算

有个疑问，filp的时候，每个流怎么知道自己要处理多少行？

### ILP

===========高性能CUDA应用设计与开发 p81

![image-20211107162338467](E:\mycode\collection\定理\并行优化\image-20211107162338467.png)

### 统一内存访问

===========高性能CUDA应用设计与开发 p81

当指针使用const关键字修饰，且线程块内所有线程均统一访问同一地址时候，可以通过全局内存广播数据。

```
__global__ void kernel(float *g_dst,const float *g_src)
{
	g_dst = g_src[0] + g_src[blockIdx.x];
}
```

### 支线：zeropadding

========== The CUDA HandBook Chapter 13.5

在数组前添加16个零，来减少if的数量

优化前

```
template<class T>
inline __device__ T
scanWarp( volatile T *sPartials )
{
const int tid = threadIdx.x;
const int lane = tid & 31;
if ( lane >= 1 ) sPartials[0] += sPartials[- 1];
if ( lane >= 2 ) sPartials[0] += sPartials[- 2];
if ( lane >= 4 ) sPartials[0] += sPartials[- 4];
if ( lane >= 8 ) sPartials[0] += sPartials[- 8];
if ( lane >= 16 ) sPartials[0] += sPartials[-16];
return sPartials[0];
}
```

优化后

```
template<class T>
__device__ T scanWarp0( volatile T *sharedPartials, int idx )
{
const int tid = threadIdx.x;
const int lane = tid & 31;
sharedPartials[idx] += sharedPartials[idx - 1];
sharedPartials[idx] += sharedPartials[idx - 2];
sharedPartials[idx] += sharedPartials[idx - 4];
sharedPartials[idx] += sharedPartials[idx - 8];
sharedPartials[idx] += sharedPartials[idx - 16];
return sharedPartials[idx];
}
```



### 原则

===========高性能CUDA应用设计与开发 p110

GPGPU编程三天重要法则

将数据放入并始终存储于GPGPU中，交给GPGPU足够多的任务，注重GPGPU上的数据重用，以避免存储带宽限制。

WARP内各线程所访问的数据在一个连续的区域中，这就意味着整合后多数据的内存传输得到了100%的利用，即所有从存储器硬件读取的字节都被利用上了。

为了最大限度地利用全局内存子系统，需要运行时报错的内存请求，也就是

从指令并行ILP的角度，可尝试在每个线程中处理多个元素，以使多个内存操作流水化进行，有点还包括可以重用索引下标。

从线程级并行的角度，启动足够多的线程以最大化吞吐量。

### 实例

reduction

================基于CUDA的并行程序

像素交换，注意共享内存的大端问题，以及寄存器替换共享内存

像素复制，两种思路，一种减少循环次数。第二种使用共享内存，一个像素一个像素复制。第三章性能最好，即仅使用全局内存。

高斯滤波，结论是kernel3最好，用个常量内存存权重就够了，不要用共享内存减少计算量，这是因为核函数消耗资源配置文件最小的解决方案总是表现最好。

第十一章，有设备依赖的流处理